{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "from argparse import Namespace\n",
    "import re\n",
    "import string\n",
    "from rhyme_finder import RhymeFinder\n",
    "import random\n",
    "\n",
    "flags = Namespace(\n",
    "    seq_size=32,\n",
    "    batch_size=16,\n",
    "    num_batches=1000,\n",
    "    embedding_size=128,\n",
    "    lstm_size=128,\n",
    "    gradients_norm=5,\n",
    "    initial_words=['heels', 'inch'],\n",
    "    predict_top_k=5,\n",
    "    checkpoint_path='checkpoint',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>The Weeknd</td>\n",
       "      <td>6 Inch Heel</td>\n",
       "      <td>https://genius.com/The-weeknd-6-inch-heel-lyrics</td>\n",
       "      <td>six inch heel she walked in the club like nobo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>The Weeknd</td>\n",
       "      <td>Acquainted</td>\n",
       "      <td>https://genius.com/The-weeknd-acquainted-lyrics</td>\n",
       "      <td>baby you're no good\\ncause they warned me 'bou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>The Weeknd</td>\n",
       "      <td>Adaptation</td>\n",
       "      <td>https://genius.com/The-weeknd-adaptation-lyrics</td>\n",
       "      <td>when the sun comes up you're searching for a l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>The Weeknd</td>\n",
       "      <td>After Hours</td>\n",
       "      <td>https://genius.com/The-weeknd-after-hours-lyrics</td>\n",
       "      <td>thought i almost died in my dream again \\nfigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>The Weeknd</td>\n",
       "      <td>Airports</td>\n",
       "      <td>https://genius.com/The-weeknd-airports-lyrics</td>\n",
       "      <td>i think i'm fuckin' gone rollin' on this floor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6462</td>\n",
       "      <td>YG</td>\n",
       "      <td>Yo Nigga Ain’t Me</td>\n",
       "      <td>https://genius.com/Yg-yo-nigga-aint-me-lyrics</td>\n",
       "      <td>hook: charlie hood and yg\\nsee shawty be rocki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6463</td>\n",
       "      <td>YG</td>\n",
       "      <td>Yo Pussy</td>\n",
       "      <td>https://genius.com/Yg-yo-pussy-lyrics</td>\n",
       "      <td>raw smooth with a banger now\\ndon't trip \\ni b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6464</td>\n",
       "      <td>YG</td>\n",
       "      <td>You Betta Kno</td>\n",
       "      <td>https://genius.com/Yg-you-betta-kno-lyrics</td>\n",
       "      <td>ay you don't even know it\\ni'm on this bitch\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6465</td>\n",
       "      <td>YG</td>\n",
       "      <td>You Broke</td>\n",
       "      <td>https://genius.com/Yg-you-broke-lyrics</td>\n",
       "      <td>bitch you broke shut up\\ndont talk to me get y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6466</td>\n",
       "      <td>YG</td>\n",
       "      <td>Youzza Flip</td>\n",
       "      <td>https://genius.com/Yg-youzza-flip-lyrics</td>\n",
       "      <td>i'm that nigga same old nigga\\nain't shit chan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8589 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          artist              title  \\\n",
       "0     The Weeknd        6 Inch Heel   \n",
       "1     The Weeknd         Acquainted   \n",
       "2     The Weeknd         Adaptation   \n",
       "3     The Weeknd        After Hours   \n",
       "4     The Weeknd           Airports   \n",
       "...          ...                ...   \n",
       "6462          YG  Yo Nigga Ain’t Me   \n",
       "6463          YG           Yo Pussy   \n",
       "6464          YG      You Betta Kno   \n",
       "6465          YG          You Broke   \n",
       "6466          YG        Youzza Flip   \n",
       "\n",
       "                                                   url  \\\n",
       "0     https://genius.com/The-weeknd-6-inch-heel-lyrics   \n",
       "1      https://genius.com/The-weeknd-acquainted-lyrics   \n",
       "2      https://genius.com/The-weeknd-adaptation-lyrics   \n",
       "3     https://genius.com/The-weeknd-after-hours-lyrics   \n",
       "4        https://genius.com/The-weeknd-airports-lyrics   \n",
       "...                                                ...   \n",
       "6462     https://genius.com/Yg-yo-nigga-aint-me-lyrics   \n",
       "6463             https://genius.com/Yg-yo-pussy-lyrics   \n",
       "6464        https://genius.com/Yg-you-betta-kno-lyrics   \n",
       "6465            https://genius.com/Yg-you-broke-lyrics   \n",
       "6466          https://genius.com/Yg-youzza-flip-lyrics   \n",
       "\n",
       "                                                 lyrics  \n",
       "0     six inch heel she walked in the club like nobo...  \n",
       "1     baby you're no good\\ncause they warned me 'bou...  \n",
       "2     when the sun comes up you're searching for a l...  \n",
       "3     thought i almost died in my dream again \\nfigh...  \n",
       "4     i think i'm fuckin' gone rollin' on this floor...  \n",
       "...                                                 ...  \n",
       "6462  hook: charlie hood and yg\\nsee shawty be rocki...  \n",
       "6463  raw smooth with a banger now\\ndon't trip \\ni b...  \n",
       "6464  ay you don't even know it\\ni'm on this bitch\\n...  \n",
       "6465  bitch you broke shut up\\ndont talk to me get y...  \n",
       "6466  i'm that nigga same old nigga\\nain't shit chan...  \n",
       "\n",
       "[8589 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('DataScraper/hiphop_lyrics.csv')\n",
    "df = df.append(pd.read_csv('DataScraper/hiphop_lyrics2.csv'))\n",
    "#df = pd.read_csv('DataScraper/lyrics.csv')\n",
    "df = df.dropna()\n",
    "\n",
    "def clean_lyrics(l):\n",
    "    l = re.sub(r'[\\(\\[].*?[\\)\\]]', '', l)\n",
    "    l = os.linesep.join([s for s in l.splitlines() if s])\n",
    "    l = l.replace('\\r', '').replace('?', '').replace(\"!\", '').replace(',', '').replace('.', '')\n",
    "    l += '\\n'\n",
    "    l = ''.join([i for i in l if i in string.printable])\n",
    "    #l = l.replace('\\n', '$')\n",
    "    return l.lower()\n",
    "\n",
    "df['lyrics'] = df['lyrics'].apply(clean_lyrics)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RhymeFinder(df['lyrics'])\n",
    "\n",
    "corpus = ''.join(list(df['lyrics']))\n",
    "\n",
    "def revert(data):\n",
    "    lines = data.split('\\n')\n",
    "    lines = [' '.join(x.split(' ')[::-1]) for x in lines]\n",
    "    lines = lines[::-1]\n",
    "    lines = ' \\n '.join(lines)\n",
    "    return lines\n",
    "\n",
    "corpus = revert(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.find_lines_ending_with_word('inch heels')\n",
    "\n",
    "def get_data_from_file(corpus, batch_size, seq_size):\n",
    "    text = corpus.split(' ')\n",
    "\n",
    "    word_counts = Counter(text)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
    "    n_vocab = len(int_to_vocab)\n",
    "\n",
    "    print('Vocabulary size', n_vocab)\n",
    "\n",
    "    int_text = [vocab_to_int[w] for w in text]\n",
    "    num_batches = int(len(int_text) / (seq_size * batch_size))\n",
    "    in_text = int_text[:num_batches * batch_size * seq_size]\n",
    "    out_text = np.zeros_like(in_text)\n",
    "    out_text[:-1] = in_text[1:]\n",
    "    out_text[-1] = in_text[0]\n",
    "    in_text = np.reshape(in_text, (batch_size, -1))\n",
    "    out_text = np.reshape(out_text, (batch_size, -1))\n",
    "    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(in_text, out_text, batch_size, seq_size):\n",
    "    num_batches = flags.num_batches #np.prod(in_text.shape) // (seq_size * batch_size)\n",
    "    for i in range(0, num_batches * seq_size, seq_size):\n",
    "        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModule(nn.Module):\n",
    "    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n",
    "        super(RNNModule, self).__init__()\n",
    "        self.seq_size = seq_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size,\n",
    "                            lstm_size,\n",
    "                            batch_first=True)\n",
    "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
    "    \n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x).float()\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.dense(output)\n",
    "\n",
    "        return logits, state\n",
    "    \n",
    "    def zero_state(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.lstm_size).float(),\n",
    "                torch.zeros(1, batch_size, self.lstm_size).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_and_train_op(net, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    return criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\n",
    "    net.eval()\n",
    "\n",
    "    state_h, state_c = net.zero_state(1)\n",
    "    state_h = state_h.to(device)\n",
    "    state_c = state_c.to(device)\n",
    "    for w in words:\n",
    "        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
    "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
    "    \n",
    "    _, top_ix = torch.topk(output[0], k=top_k)\n",
    "    choices = top_ix.tolist()[0]\n",
    "    return [int_to_vocab[choice] for choice in choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 74389\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(\n",
    "    corpus, flags.batch_size, flags.seq_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModule(n_vocab, flags.seq_size,\n",
    "                flags.embedding_size, flags.lstm_size)\n",
    "model.to(device)\n",
    "criterion, optimizer = get_loss_and_train_op(model, 0.01)\n",
    "\n",
    "iteration = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(in_text, out_text, flags.batch_size, flags.seq_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/200 Iteration: 100 Loss: 6.287078380584717 Perplexity: 537.5804270799314\n",
      "Epoch: 0/200 Iteration: 200 Loss: 5.855099678039551 Perplexity: 349.00968692158983\n",
      "Epoch: 0/200 Iteration: 300 Loss: 5.998870372772217 Perplexity: 402.97332664544354\n",
      "Epoch: 0/200 Iteration: 400 Loss: 5.381933212280273 Perplexity: 217.44223135407944\n",
      "Epoch: 0/200 Iteration: 500 Loss: 5.101699352264404 Perplexity: 164.30087526477155\n",
      "Epoch: 0/200 Iteration: 600 Loss: 5.28031063079834 Perplexity: 196.43088335794104\n",
      "Epoch: 0/200 Iteration: 700 Loss: 6.15194034576416 Perplexity: 469.6277434962251\n",
      "Epoch: 0/200 Iteration: 800 Loss: 5.652627468109131 Perplexity: 285.03941475976507\n",
      "Epoch: 0/200 Iteration: 900 Loss: 5.957114219665527 Perplexity: 386.4931789682968\n",
      "Epoch: 0/200 Iteration: 1000 Loss: 5.523105621337891 Perplexity: 250.41151418555282\n",
      "[\"i'm\", 'my', '\\n', '26', 'the']\n",
      "Epoch: 1/200 Iteration: 1100 Loss: 5.457187175750732 Perplexity: 234.43706580073066\n",
      "Epoch: 1/200 Iteration: 1200 Loss: 5.203061103820801 Perplexity: 181.8279851873892\n",
      "Epoch: 1/200 Iteration: 1300 Loss: 5.301990985870361 Perplexity: 200.73607506645138\n",
      "Epoch: 1/200 Iteration: 1400 Loss: 4.755570411682129 Perplexity: 116.22993317085074\n",
      "Epoch: 1/200 Iteration: 1500 Loss: 4.550206661224365 Perplexity: 94.65196718525151\n",
      "Epoch: 1/200 Iteration: 1600 Loss: 4.695443153381348 Perplexity: 109.4472998375029\n",
      "Epoch: 1/200 Iteration: 1700 Loss: 5.480474948883057 Perplexity: 239.96064935619412\n",
      "Epoch: 1/200 Iteration: 1800 Loss: 4.985878944396973 Perplexity: 146.33213633008606\n",
      "Epoch: 1/200 Iteration: 1900 Loss: 5.139286518096924 Perplexity: 170.5940091517031\n",
      "Epoch: 1/200 Iteration: 2000 Loss: 4.958276271820068 Perplexity: 142.34821467116444\n",
      "['26', 'my', 'an', 'kings', 'written']\n",
      "Epoch: 2/200 Iteration: 2100 Loss: 5.0665788650512695 Perplexity: 158.63070092896913\n",
      "Epoch: 2/200 Iteration: 2200 Loss: 4.968944549560547 Perplexity: 143.87495432235647\n",
      "Epoch: 2/200 Iteration: 2300 Loss: 4.976428985595703 Perplexity: 144.95581699327204\n",
      "Epoch: 2/200 Iteration: 2400 Loss: 4.4692888259887695 Perplexity: 87.2946192682299\n",
      "Epoch: 2/200 Iteration: 2500 Loss: 4.188981533050537 Perplexity: 65.9555831601765\n",
      "Epoch: 2/200 Iteration: 2600 Loss: 4.443363189697266 Perplexity: 85.0605358687814\n",
      "Epoch: 2/200 Iteration: 2700 Loss: 5.198987007141113 Perplexity: 181.08870736388647\n",
      "Epoch: 2/200 Iteration: 2800 Loss: 4.604610919952393 Perplexity: 99.94408903244002\n",
      "Epoch: 2/200 Iteration: 2900 Loss: 4.694918155670166 Perplexity: 109.38985533602852\n",
      "Epoch: 2/200 Iteration: 3000 Loss: 4.609859943389893 Perplexity: 100.47007715251695\n",
      "['26', '\\n', 'an', 'my', 'they']\n",
      "Epoch: 3/200 Iteration: 3100 Loss: 4.844330787658691 Perplexity: 127.01825136375275\n",
      "Epoch: 3/200 Iteration: 3200 Loss: 4.705373764038086 Perplexity: 110.53959295189784\n",
      "Epoch: 3/200 Iteration: 3300 Loss: 4.738314628601074 Perplexity: 114.24149998216691\n",
      "Epoch: 3/200 Iteration: 3400 Loss: 4.203927993774414 Perplexity: 66.94878966455319\n",
      "Epoch: 3/200 Iteration: 3500 Loss: 4.014229774475098 Perplexity: 55.3806234013425\n",
      "Epoch: 3/200 Iteration: 3600 Loss: 4.229462623596191 Perplexity: 68.68031507481855\n",
      "Epoch: 3/200 Iteration: 3700 Loss: 4.8899006843566895 Perplexity: 132.94037033722398\n",
      "Epoch: 3/200 Iteration: 3800 Loss: 4.367892742156982 Perplexity: 78.8772417615476\n",
      "Epoch: 3/200 Iteration: 3900 Loss: 4.485936164855957 Perplexity: 88.7600059284194\n",
      "Epoch: 3/200 Iteration: 4000 Loss: 4.448374271392822 Perplexity: 85.48785092422413\n",
      "['26', 'oklahoma', 'responsible', 'bug-', 'ayaa']\n",
      "Epoch: 4/200 Iteration: 4100 Loss: 4.531644344329834 Perplexity: 92.9112135682997\n",
      "Epoch: 4/200 Iteration: 4200 Loss: 4.572948455810547 Perplexity: 96.82918587490009\n",
      "Epoch: 4/200 Iteration: 4300 Loss: 4.5137224197387695 Perplexity: 91.26089838971994\n",
      "Epoch: 4/200 Iteration: 4400 Loss: 3.98974871635437 Perplexity: 54.04130796012697\n",
      "Epoch: 4/200 Iteration: 4500 Loss: 3.8135037422180176 Perplexity: 45.30891182721785\n",
      "Epoch: 4/200 Iteration: 4600 Loss: 4.118462085723877 Perplexity: 61.46464218897154\n",
      "Epoch: 4/200 Iteration: 4700 Loss: 4.655801773071289 Perplexity: 105.19352753427839\n",
      "Epoch: 4/200 Iteration: 4800 Loss: 4.195627689361572 Perplexity: 66.39539418292581\n",
      "Epoch: 4/200 Iteration: 4900 Loss: 4.2548017501831055 Perplexity: 70.44285052181625\n",
      "Epoch: 4/200 Iteration: 5000 Loss: 4.214965343475342 Perplexity: 67.69181986705556\n",
      "['\\n', '26', 'see', 'got', '7']\n",
      "Epoch: 5/200 Iteration: 5100 Loss: 4.330366134643555 Perplexity: 75.97209749444127\n",
      "Epoch: 5/200 Iteration: 5200 Loss: 4.409584045410156 Perplexity: 82.23525025932453\n",
      "Epoch: 5/200 Iteration: 5300 Loss: 4.367168426513672 Perplexity: 78.82013042725181\n",
      "Epoch: 5/200 Iteration: 5400 Loss: 3.8678550720214844 Perplexity: 47.83966332140124\n",
      "Epoch: 5/200 Iteration: 5500 Loss: 3.718637228012085 Perplexity: 41.20819844785442\n",
      "Epoch: 5/200 Iteration: 5600 Loss: 4.073330879211426 Perplexity: 58.75233395985486\n",
      "Epoch: 5/200 Iteration: 5700 Loss: 4.482412815093994 Perplexity: 88.44782366925625\n",
      "Epoch: 5/200 Iteration: 5800 Loss: 4.0195512771606445 Perplexity: 55.676117075364814\n",
      "Epoch: 5/200 Iteration: 5900 Loss: 4.101812362670898 Perplexity: 60.44974524066718\n",
      "Epoch: 5/200 Iteration: 6000 Loss: 4.190662860870361 Perplexity: 66.06656939299162\n",
      "['see', 'take', 'an', 'got', '\\n']\n",
      "Epoch: 6/200 Iteration: 6100 Loss: 4.230886459350586 Perplexity: 68.77817421418337\n",
      "Epoch: 6/200 Iteration: 6200 Loss: 4.358365058898926 Perplexity: 78.12929315251448\n",
      "Epoch: 6/200 Iteration: 6300 Loss: 4.290895462036133 Perplexity: 73.03183646498262\n",
      "Epoch: 6/200 Iteration: 6400 Loss: 3.7819907665252686 Perplexity: 43.903356127178235\n",
      "Epoch: 6/200 Iteration: 6500 Loss: 3.5988569259643555 Perplexity: 36.55642385298785\n",
      "Epoch: 6/200 Iteration: 6600 Loss: 3.9808547496795654 Perplexity: 53.562797450612344\n",
      "Epoch: 6/200 Iteration: 6700 Loss: 4.388790130615234 Perplexity: 80.54291360265749\n",
      "Epoch: 6/200 Iteration: 6800 Loss: 3.9253110885620117 Perplexity: 50.66883810331111\n",
      "Epoch: 6/200 Iteration: 6900 Loss: 4.136641979217529 Perplexity: 62.592281965400744\n",
      "Epoch: 6/200 Iteration: 7000 Loss: 3.993922710418701 Perplexity: 54.2673474744008\n",
      "['the', '\\n', 'every', 'got', 'and']\n",
      "Epoch: 7/200 Iteration: 7100 Loss: 4.188257694244385 Perplexity: 65.907859223905\n",
      "Epoch: 7/200 Iteration: 7200 Loss: 4.254392623901367 Perplexity: 70.41403639501742\n",
      "Epoch: 7/200 Iteration: 7300 Loss: 4.149789333343506 Perplexity: 63.420638276888056\n",
      "Epoch: 7/200 Iteration: 7400 Loss: 3.607555627822876 Perplexity: 36.87580436915453\n",
      "Epoch: 7/200 Iteration: 7500 Loss: 3.506608724594116 Perplexity: 33.33502761886208\n",
      "Epoch: 7/200 Iteration: 7600 Loss: 3.8244471549987793 Perplexity: 45.807468934581884\n",
      "Epoch: 7/200 Iteration: 7700 Loss: 4.170954704284668 Perplexity: 64.77726572898383\n",
      "Epoch: 7/200 Iteration: 7800 Loss: 3.8895647525787354 Perplexity: 48.889602818675534\n",
      "Epoch: 7/200 Iteration: 7900 Loss: 3.893017292022705 Perplexity: 49.05868781916568\n",
      "Epoch: 7/200 Iteration: 8000 Loss: 3.8898820877075195 Perplexity: 48.90511966897283\n",
      "['\\n', 'the', 'every', 'a', '10']\n",
      "Epoch: 8/200 Iteration: 8100 Loss: 4.004805088043213 Perplexity: 54.86113026664498\n",
      "Epoch: 8/200 Iteration: 8200 Loss: 4.105567932128906 Perplexity: 60.677195292536105\n",
      "Epoch: 8/200 Iteration: 8300 Loss: 4.102179527282715 Perplexity: 60.47194432302221\n",
      "Epoch: 8/200 Iteration: 8400 Loss: 3.5796098709106445 Perplexity: 35.859548264874284\n",
      "Epoch: 8/200 Iteration: 8500 Loss: 3.4926679134368896 Perplexity: 32.87353456178105\n",
      "Epoch: 8/200 Iteration: 8600 Loss: 3.7975125312805176 Perplexity: 44.590129874857226\n",
      "Epoch: 8/200 Iteration: 8700 Loss: 4.157425403594971 Perplexity: 63.90677645660254\n",
      "Epoch: 8/200 Iteration: 8800 Loss: 3.8642241954803467 Perplexity: 47.66627837026906\n",
      "Epoch: 8/200 Iteration: 8900 Loss: 3.865180253982544 Perplexity: 47.71187191254704\n",
      "Epoch: 8/200 Iteration: 9000 Loss: 3.8302230834960938 Perplexity: 46.07281517254776\n",
      "['7', 'my', '8', 'the', '26']\n",
      "Epoch: 9/200 Iteration: 9100 Loss: 3.9964005947113037 Perplexity: 54.40198241832801\n",
      "Epoch: 9/200 Iteration: 9200 Loss: 4.108248233795166 Perplexity: 60.840046628041726\n",
      "Epoch: 9/200 Iteration: 9300 Loss: 4.078146457672119 Perplexity: 59.03594275592324\n",
      "Epoch: 9/200 Iteration: 9400 Loss: 3.527710437774658 Perplexity: 34.045928039847134\n",
      "Epoch: 9/200 Iteration: 9500 Loss: 3.4750428199768066 Perplexity: 32.29921155848979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/200 Iteration: 9600 Loss: 3.7710254192352295 Perplexity: 43.42457040329252\n",
      "Epoch: 9/200 Iteration: 9700 Loss: 4.087973594665527 Perplexity: 59.61895704162075\n",
      "Epoch: 9/200 Iteration: 9800 Loss: 3.847871780395508 Perplexity: 46.89315802095715\n",
      "Epoch: 9/200 Iteration: 9900 Loss: 3.7519211769104004 Perplexity: 42.602851042254215\n",
      "Epoch: 9/200 Iteration: 10000 Loss: 3.7981345653533936 Perplexity: 44.617875083291246\n",
      "['26', '7', '8', 'see', 'the']\n",
      "Epoch: 10/200 Iteration: 10100 Loss: 3.871372699737549 Perplexity: 48.00824177131337\n",
      "Epoch: 10/200 Iteration: 10200 Loss: 4.034018516540527 Perplexity: 56.487451522625065\n",
      "Epoch: 10/200 Iteration: 10300 Loss: 4.066092491149902 Perplexity: 58.328597202787925\n",
      "Epoch: 10/200 Iteration: 10400 Loss: 3.496262311935425 Perplexity: 32.99190775782503\n",
      "Epoch: 10/200 Iteration: 10500 Loss: 3.439624071121216 Perplexity: 31.17523629352623\n",
      "Epoch: 10/200 Iteration: 10600 Loss: 3.702659845352173 Perplexity: 40.555031139352124\n",
      "Epoch: 10/200 Iteration: 10700 Loss: 4.007200717926025 Perplexity: 54.99271478068284\n",
      "Epoch: 10/200 Iteration: 10800 Loss: 3.7234079837799072 Perplexity: 41.40526239649961\n",
      "Epoch: 10/200 Iteration: 10900 Loss: 3.663646936416626 Perplexity: 39.00332646304768\n",
      "Epoch: 10/200 Iteration: 11000 Loss: 3.769418478012085 Perplexity: 43.35484570776183\n",
      "['26', '7', '10', \"mind's\", 'a']\n",
      "Epoch: 11/200 Iteration: 11100 Loss: 3.8616654872894287 Perplexity: 47.54447017564152\n",
      "Epoch: 11/200 Iteration: 11200 Loss: 4.0061936378479 Perplexity: 54.93736059091448\n",
      "Epoch: 11/200 Iteration: 11300 Loss: 4.0039753913879395 Perplexity: 54.81563104824301\n",
      "Epoch: 11/200 Iteration: 11400 Loss: 3.490859270095825 Perplexity: 32.81413179779854\n",
      "Epoch: 11/200 Iteration: 11500 Loss: 3.3441245555877686 Perplexity: 28.335758423801806\n",
      "Epoch: 11/200 Iteration: 11600 Loss: 3.730557680130005 Perplexity: 41.70235825694354\n",
      "Epoch: 11/200 Iteration: 11700 Loss: 3.9985649585723877 Perplexity: 54.5198556172771\n",
      "Epoch: 11/200 Iteration: 11800 Loss: 3.6693239212036133 Perplexity: 39.22537744769369\n",
      "Epoch: 11/200 Iteration: 11900 Loss: 3.633082389831543 Perplexity: 37.829241560992195\n",
      "Epoch: 11/200 Iteration: 12000 Loss: 3.7098684310913086 Perplexity: 40.84843178983152\n",
      "['26', 'an', '4', '\\n', 'them']\n",
      "Epoch: 12/200 Iteration: 12100 Loss: 3.775850296020508 Perplexity: 43.634594868588096\n",
      "Epoch: 12/200 Iteration: 12200 Loss: 3.9786276817321777 Perplexity: 53.443642193913306\n",
      "Epoch: 12/200 Iteration: 12300 Loss: 3.8716485500335693 Perplexity: 48.02148668574021\n",
      "Epoch: 12/200 Iteration: 12400 Loss: 3.4485397338867188 Perplexity: 31.454426922320454\n",
      "Epoch: 12/200 Iteration: 12500 Loss: 3.352466583251953 Perplexity: 28.573124786326037\n",
      "Epoch: 12/200 Iteration: 12600 Loss: 3.6679723262786865 Perplexity: 39.17239643910373\n",
      "Epoch: 12/200 Iteration: 12700 Loss: 3.888474941253662 Perplexity: 48.83635139811652\n",
      "Epoch: 12/200 Iteration: 12800 Loss: 3.64516544342041 Perplexity: 38.289107008399164\n",
      "Epoch: 12/200 Iteration: 12900 Loss: 3.55743408203125 Perplexity: 35.07308692287246\n",
      "Epoch: 12/200 Iteration: 13000 Loss: 3.6155450344085693 Perplexity: 37.1716002066393\n",
      "['7', 'ten', '8', 'see', 'better']\n",
      "Epoch: 13/200 Iteration: 13100 Loss: 3.682098865509033 Perplexity: 39.72969390864586\n",
      "Epoch: 13/200 Iteration: 13200 Loss: 3.962205171585083 Perplexity: 52.57313099177832\n",
      "Epoch: 13/200 Iteration: 13300 Loss: 3.883908748626709 Perplexity: 48.613863558176924\n",
      "Epoch: 13/200 Iteration: 13400 Loss: 3.4077396392822266 Perplexity: 30.19691114799682\n",
      "Epoch: 13/200 Iteration: 13500 Loss: 3.3320138454437256 Perplexity: 27.99466189642882\n",
      "Epoch: 13/200 Iteration: 13600 Loss: 3.6262238025665283 Perplexity: 37.570674123830926\n",
      "Epoch: 13/200 Iteration: 13700 Loss: 3.9333598613739014 Perplexity: 51.07830571520416\n",
      "Epoch: 13/200 Iteration: 13800 Loss: 3.6404874324798584 Perplexity: 38.11040844969955\n",
      "Epoch: 13/200 Iteration: 13900 Loss: 3.5715830326080322 Perplexity: 35.57286160235918\n",
      "Epoch: 13/200 Iteration: 14000 Loss: 3.566657781600952 Perplexity: 35.39808708741327\n",
      "['ten', 'a', 'the', '26', '10']\n",
      "Epoch: 14/200 Iteration: 14100 Loss: 3.6831772327423096 Perplexity: 39.772560217402415\n",
      "Epoch: 14/200 Iteration: 14200 Loss: 3.9268343448638916 Perplexity: 50.74607854381965\n",
      "Epoch: 14/200 Iteration: 14300 Loss: 3.8429224491119385 Perplexity: 46.661641645198856\n",
      "Epoch: 14/200 Iteration: 14400 Loss: 3.4264965057373047 Perplexity: 30.76865588508931\n",
      "Epoch: 14/200 Iteration: 14500 Loss: 3.2908732891082764 Perplexity: 26.866315474982866\n",
      "Epoch: 14/200 Iteration: 14600 Loss: 3.5849266052246094 Perplexity: 36.050711687870994\n",
      "Epoch: 14/200 Iteration: 14700 Loss: 3.805762529373169 Perplexity: 44.95952000005879\n",
      "Epoch: 14/200 Iteration: 14800 Loss: 3.5381739139556885 Perplexity: 34.404037062295096\n",
      "Epoch: 14/200 Iteration: 14900 Loss: 3.566049098968506 Perplexity: 35.37654744265021\n",
      "Epoch: 14/200 Iteration: 15000 Loss: 3.6632797718048096 Perplexity: 38.98900845052193\n",
      "['ten', 'a', '26', 'see', 'clothes']\n",
      "Epoch: 15/200 Iteration: 15100 Loss: 3.7036702632904053 Perplexity: 40.596029379491455\n",
      "Epoch: 15/200 Iteration: 15200 Loss: 3.941528081893921 Perplexity: 51.497233196872855\n",
      "Epoch: 15/200 Iteration: 15300 Loss: 3.79960560798645 Perplexity: 44.68355817920768\n",
      "Epoch: 15/200 Iteration: 15400 Loss: 3.439042091369629 Perplexity: 31.157098215761405\n",
      "Epoch: 15/200 Iteration: 15500 Loss: 3.260140895843506 Perplexity: 26.053207672602117\n",
      "Epoch: 15/200 Iteration: 15600 Loss: 3.530639171600342 Perplexity: 34.14578565773863\n",
      "Epoch: 15/200 Iteration: 15700 Loss: 3.8511292934417725 Perplexity: 47.046162166191706\n",
      "Epoch: 15/200 Iteration: 15800 Loss: 3.4970133304595947 Perplexity: 33.01669459822197\n",
      "Epoch: 15/200 Iteration: 15900 Loss: 3.5440454483032227 Perplexity: 34.60663574812412\n",
      "Epoch: 15/200 Iteration: 16000 Loss: 3.6083250045776367 Perplexity: 36.904186672791546\n",
      "['a', '26', '\\n', 'ten', 'fit']\n",
      "Epoch: 16/200 Iteration: 16100 Loss: 3.668389320373535 Perplexity: 39.188734503301305\n",
      "Epoch: 16/200 Iteration: 16200 Loss: 3.9172589778900146 Perplexity: 50.26248520607078\n",
      "Epoch: 16/200 Iteration: 16300 Loss: 3.8447718620300293 Perplexity: 46.7480181363164\n",
      "Epoch: 16/200 Iteration: 16400 Loss: 3.3690030574798584 Perplexity: 29.049551883441595\n",
      "Epoch: 16/200 Iteration: 16500 Loss: 3.294036626815796 Perplexity: 26.951437267292288\n",
      "Epoch: 16/200 Iteration: 16600 Loss: 3.509338855743408 Perplexity: 33.426160962510885\n",
      "Epoch: 16/200 Iteration: 16700 Loss: 3.8160173892974854 Perplexity: 45.42294570152091\n",
      "Epoch: 16/200 Iteration: 16800 Loss: 3.5877482891082764 Perplexity: 36.152579051195936\n",
      "Epoch: 16/200 Iteration: 16900 Loss: 3.5124292373657227 Perplexity: 33.52962033819022\n",
      "Epoch: 16/200 Iteration: 17000 Loss: 3.6246705055236816 Perplexity: 37.51236100733682\n",
      "['ten', '10', '30', 'a', 'the']\n",
      "Epoch: 17/200 Iteration: 17100 Loss: 3.658536434173584 Perplexity: 38.80450833862235\n",
      "Epoch: 17/200 Iteration: 17200 Loss: 3.8979086875915527 Perplexity: 49.29924110840638\n",
      "Epoch: 17/200 Iteration: 17300 Loss: 3.7633635997772217 Perplexity: 43.09313052247283\n",
      "Epoch: 17/200 Iteration: 17400 Loss: 3.32877516746521 Perplexity: 27.904142861552742\n",
      "Epoch: 17/200 Iteration: 17500 Loss: 3.241373062133789 Perplexity: 25.56880521419554\n",
      "Epoch: 17/200 Iteration: 17600 Loss: 3.502479314804077 Perplexity: 33.19765745351758\n",
      "Epoch: 17/200 Iteration: 17700 Loss: 3.900993585586548 Perplexity: 49.451559060333054\n",
      "Epoch: 17/200 Iteration: 17800 Loss: 3.5431385040283203 Perplexity: 34.57526368646501\n",
      "Epoch: 17/200 Iteration: 17900 Loss: 3.5267891883850098 Perplexity: 34.0145776923819\n",
      "Epoch: 17/200 Iteration: 18000 Loss: 3.5319290161132812 Perplexity: 34.189856828377984\n",
      "['30', 'heavy', 'ten', 'a', 'fifty']\n",
      "Epoch: 18/200 Iteration: 18100 Loss: 3.586416721343994 Perplexity: 36.104471478690265\n",
      "Epoch: 18/200 Iteration: 18200 Loss: 3.976917266845703 Perplexity: 53.35230952336337\n",
      "Epoch: 18/200 Iteration: 18300 Loss: 3.789268732070923 Perplexity: 44.22404882003791\n",
      "Epoch: 18/200 Iteration: 18400 Loss: 3.310924530029297 Perplexity: 27.4104555501628\n",
      "Epoch: 18/200 Iteration: 18500 Loss: 3.2223241329193115 Perplexity: 25.086356507627176\n",
      "Epoch: 18/200 Iteration: 18600 Loss: 3.4456303119659424 Perplexity: 31.36304572082239\n",
      "Epoch: 18/200 Iteration: 18700 Loss: 3.7909252643585205 Perplexity: 44.297368095904204\n",
      "Epoch: 18/200 Iteration: 18800 Loss: 3.446889638900757 Perplexity: 31.40256692889428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/200 Iteration: 18900 Loss: 3.4602503776550293 Perplexity: 31.82494377200754\n",
      "Epoch: 18/200 Iteration: 19000 Loss: 3.540494203567505 Perplexity: 34.483957075027206\n",
      "['30', '10', 'a', '7', 'heavy']\n",
      "Epoch: 19/200 Iteration: 19100 Loss: 3.587719678878784 Perplexity: 36.15154473240867\n",
      "Epoch: 19/200 Iteration: 19200 Loss: 3.81648850440979 Perplexity: 45.444350179276874\n",
      "Epoch: 19/200 Iteration: 19300 Loss: 3.7240121364593506 Perplexity: 41.430285054711405\n",
      "Epoch: 19/200 Iteration: 19400 Loss: 3.327667236328125 Perplexity: 27.873244112826377\n",
      "Epoch: 19/200 Iteration: 19500 Loss: 3.2024176120758057 Perplexity: 24.591912090632203\n",
      "Epoch: 19/200 Iteration: 19600 Loss: 3.487912654876709 Perplexity: 32.71758349285047\n",
      "Epoch: 19/200 Iteration: 19700 Loss: 3.7401533126831055 Perplexity: 42.1044448155997\n",
      "Epoch: 19/200 Iteration: 19800 Loss: 3.4957966804504395 Perplexity: 32.97654926280017\n",
      "Epoch: 19/200 Iteration: 19900 Loss: 3.5554006099700928 Perplexity: 35.00183924514811\n",
      "Epoch: 19/200 Iteration: 20000 Loss: 3.4717624187469482 Perplexity: 32.19343078172414\n",
      "['the', 'a', '26', '10', 'ten']\n",
      "Epoch: 20/200 Iteration: 20100 Loss: 3.5403356552124023 Perplexity: 34.478490133754256\n",
      "Epoch: 20/200 Iteration: 20200 Loss: 3.8535094261169434 Perplexity: 47.15827163878901\n",
      "Epoch: 20/200 Iteration: 20300 Loss: 3.703322649002075 Perplexity: 40.58192007207018\n",
      "Epoch: 20/200 Iteration: 20400 Loss: 3.3409078121185303 Perplexity: 28.244756002139063\n",
      "Epoch: 20/200 Iteration: 20500 Loss: 3.2020866870880127 Perplexity: 24.583775358822\n",
      "Epoch: 20/200 Iteration: 20600 Loss: 3.5078048706054688 Perplexity: 33.374925035997904\n",
      "Epoch: 20/200 Iteration: 20700 Loss: 3.7470855712890625 Perplexity: 42.39733774735234\n",
      "Epoch: 20/200 Iteration: 20800 Loss: 3.4340124130249023 Perplexity: 31.000781474582315\n",
      "Epoch: 20/200 Iteration: 20900 Loss: 3.428691864013672 Perplexity: 30.836278309003934\n",
      "Epoch: 20/200 Iteration: 21000 Loss: 3.4778404235839844 Perplexity: 32.389698463483775\n",
      "['10', 'ten', '7', 'hundred', 'fifty']\n",
      "Epoch: 21/200 Iteration: 21100 Loss: 3.581578016281128 Perplexity: 35.930194567071126\n",
      "Epoch: 21/200 Iteration: 21200 Loss: 3.8152668476104736 Perplexity: 45.38886667768883\n",
      "Epoch: 21/200 Iteration: 21300 Loss: 3.7417337894439697 Perplexity: 42.17104252636404\n",
      "Epoch: 21/200 Iteration: 21400 Loss: 3.287465810775757 Perplexity: 26.774924881288435\n",
      "Epoch: 21/200 Iteration: 21500 Loss: 3.165013551712036 Perplexity: 23.689065058043937\n",
      "Epoch: 21/200 Iteration: 21600 Loss: 3.497846841812134 Perplexity: 33.044225860208684\n",
      "Epoch: 21/200 Iteration: 21700 Loss: 3.791200876235962 Perplexity: 44.309578659301685\n",
      "Epoch: 21/200 Iteration: 21800 Loss: 3.5447516441345215 Perplexity: 34.63108344143824\n",
      "Epoch: 21/200 Iteration: 21900 Loss: 3.439364433288574 Perplexity: 31.167143073441416\n",
      "Epoch: 21/200 Iteration: 22000 Loss: 3.4473977088928223 Perplexity: 31.418525684563818\n",
      "['10', '24', 'ten', '7', '26']\n",
      "Epoch: 22/200 Iteration: 22100 Loss: 3.5273079872131348 Perplexity: 34.0322289937692\n",
      "Epoch: 22/200 Iteration: 22200 Loss: 3.7790136337280273 Perplexity: 43.77284437754545\n",
      "Epoch: 22/200 Iteration: 22300 Loss: 3.688049077987671 Perplexity: 39.9667987416233\n",
      "Epoch: 22/200 Iteration: 22400 Loss: 3.3015482425689697 Perplexity: 27.154648374412677\n",
      "Epoch: 22/200 Iteration: 22500 Loss: 3.138625383377075 Perplexity: 23.072129717482763\n",
      "Epoch: 22/200 Iteration: 22600 Loss: 3.455648899078369 Perplexity: 31.67883838324346\n",
      "Epoch: 22/200 Iteration: 22700 Loss: 3.835705280303955 Perplexity: 46.32608902703997\n",
      "Epoch: 22/200 Iteration: 22800 Loss: 3.4896240234375 Perplexity: 32.77362327530876\n",
      "Epoch: 22/200 Iteration: 22900 Loss: 3.392120599746704 Perplexity: 29.728928631466736\n",
      "Epoch: 22/200 Iteration: 23000 Loss: 3.3926994800567627 Perplexity: 29.746143104969793\n",
      "['7', '10', 'ten', '30', 'a']\n",
      "Epoch: 23/200 Iteration: 23100 Loss: 3.392667055130005 Perplexity: 29.745178604095305\n",
      "Epoch: 23/200 Iteration: 23200 Loss: 3.7809395790100098 Perplexity: 43.857229715340964\n",
      "Epoch: 23/200 Iteration: 23300 Loss: 3.5970895290374756 Perplexity: 36.491871203692895\n",
      "Epoch: 23/200 Iteration: 23400 Loss: 3.2959892749786377 Perplexity: 27.004115355906954\n",
      "Epoch: 23/200 Iteration: 23500 Loss: 3.129908323287964 Perplexity: 22.87188262732579\n",
      "Epoch: 23/200 Iteration: 23600 Loss: 3.492844343185425 Perplexity: 32.87933494288209\n",
      "Epoch: 23/200 Iteration: 23700 Loss: 3.673905611038208 Perplexity: 39.40550829753873\n",
      "Epoch: 23/200 Iteration: 23800 Loss: 3.479978084564209 Perplexity: 32.45901071470377\n",
      "Epoch: 23/200 Iteration: 23900 Loss: 3.3977718353271484 Perplexity: 29.897409424654093\n",
      "Epoch: 23/200 Iteration: 24000 Loss: 3.4190757274627686 Perplexity: 30.54117360366078\n",
      "['7', 'ten', 'a', '10', 'five']\n",
      "Epoch: 24/200 Iteration: 24100 Loss: 3.4777441024780273 Perplexity: 32.38657880215318\n",
      "Epoch: 24/200 Iteration: 24200 Loss: 3.798557996749878 Perplexity: 44.63677169287248\n",
      "Epoch: 24/200 Iteration: 24300 Loss: 3.5870039463043213 Perplexity: 36.12567915175003\n",
      "Epoch: 24/200 Iteration: 24400 Loss: 3.323453903198242 Perplexity: 27.75605190823558\n",
      "Epoch: 24/200 Iteration: 24500 Loss: 3.1920793056488037 Perplexity: 24.33898304954176\n",
      "Epoch: 24/200 Iteration: 24600 Loss: 3.4655978679656982 Perplexity: 31.995583190154914\n",
      "Epoch: 24/200 Iteration: 24700 Loss: 3.625403881072998 Perplexity: 37.53988174598167\n",
      "Epoch: 24/200 Iteration: 24800 Loss: 3.3818461894989014 Perplexity: 29.425045207208832\n",
      "Epoch: 24/200 Iteration: 24900 Loss: 3.397216320037842 Perplexity: 29.8808055688829\n",
      "Epoch: 24/200 Iteration: 25000 Loss: 3.4350879192352295 Perplexity: 31.03414094352562\n",
      "['five', '7', '30', 'hundred', 'ten']\n",
      "Epoch: 25/200 Iteration: 25100 Loss: 3.444711208343506 Perplexity: 31.334233074821693\n",
      "Epoch: 25/200 Iteration: 25200 Loss: 3.7786433696746826 Perplexity: 43.75663986691881\n",
      "Epoch: 25/200 Iteration: 25300 Loss: 3.6066505908966064 Perplexity: 36.842445502294716\n",
      "Epoch: 25/200 Iteration: 25400 Loss: 3.2621419429779053 Perplexity: 26.105393564958952\n",
      "Epoch: 25/200 Iteration: 25500 Loss: 3.1421899795532227 Perplexity: 23.15451929840264\n",
      "Epoch: 25/200 Iteration: 25600 Loss: 3.535729169845581 Perplexity: 34.32003072416388\n",
      "Epoch: 25/200 Iteration: 25700 Loss: 3.6681907176971436 Perplexity: 39.180952288554465\n",
      "Epoch: 25/200 Iteration: 25800 Loss: 3.420341730117798 Perplexity: 30.5798632959898\n",
      "Epoch: 25/200 Iteration: 25900 Loss: 3.3280954360961914 Perplexity: 27.885181985205435\n",
      "Epoch: 25/200 Iteration: 26000 Loss: 3.4463305473327637 Perplexity: 31.385014925556735\n",
      "['million', 'own', 'an', 'em', '\\n']\n",
      "Epoch: 26/200 Iteration: 26100 Loss: 3.4543933868408203 Perplexity: 31.639090171386464\n",
      "Epoch: 26/200 Iteration: 26200 Loss: 3.883070468902588 Perplexity: 48.57312861806751\n",
      "Epoch: 26/200 Iteration: 26300 Loss: 3.551605224609375 Perplexity: 34.86924555811531\n",
      "Epoch: 26/200 Iteration: 26400 Loss: 3.250342607498169 Perplexity: 25.799177394835624\n",
      "Epoch: 26/200 Iteration: 26500 Loss: 3.1545186042785645 Perplexity: 23.44174961680951\n",
      "Epoch: 26/200 Iteration: 26600 Loss: 3.481694221496582 Perplexity: 32.51476264707163\n",
      "Epoch: 26/200 Iteration: 26700 Loss: 3.641406536102295 Perplexity: 38.14545196600267\n",
      "Epoch: 26/200 Iteration: 26800 Loss: 3.3711659908294678 Perplexity: 29.11245212799774\n",
      "Epoch: 26/200 Iteration: 26900 Loss: 3.373080253601074 Perplexity: 29.168234385211644\n",
      "Epoch: 26/200 Iteration: 27000 Loss: 3.4683315753936768 Perplexity: 32.0831694165978\n",
      "['million', 'a', 'an', '\\n', 'the']\n",
      "Epoch: 27/200 Iteration: 27100 Loss: 3.537825584411621 Perplexity: 34.392055206689406\n",
      "Epoch: 27/200 Iteration: 27200 Loss: 3.7324695587158203 Perplexity: 41.78216436816105\n",
      "Epoch: 27/200 Iteration: 27300 Loss: 3.571904182434082 Perplexity: 35.58428765531413\n",
      "Epoch: 27/200 Iteration: 27400 Loss: 3.297048330307007 Perplexity: 27.03272935739344\n",
      "Epoch: 27/200 Iteration: 27500 Loss: 3.189887046813965 Perplexity: 24.285684142766247\n",
      "Epoch: 27/200 Iteration: 27600 Loss: 3.4726691246032715 Perplexity: 32.222633991308946\n",
      "Epoch: 27/200 Iteration: 27700 Loss: 3.6888630390167236 Perplexity: 39.999343401500575\n",
      "Epoch: 27/200 Iteration: 27800 Loss: 3.379701852798462 Perplexity: 29.362015605354944\n",
      "Epoch: 27/200 Iteration: 27900 Loss: 3.410735845565796 Perplexity: 30.287523001037442\n",
      "Epoch: 27/200 Iteration: 28000 Loss: 3.4651992321014404 Perplexity: 31.982831145077597\n",
      "['an', 'ten', 'a', 'the', 'old']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28/200 Iteration: 28100 Loss: 3.4644856452941895 Perplexity: 31.960016759704246\n",
      "Epoch: 28/200 Iteration: 28200 Loss: 3.726583957672119 Perplexity: 41.53697347363018\n",
      "Epoch: 28/200 Iteration: 28300 Loss: 3.5900495052337646 Perplexity: 36.235869747276844\n",
      "Epoch: 28/200 Iteration: 28400 Loss: 3.2328438758850098 Perplexity: 25.35165150103581\n",
      "Epoch: 28/200 Iteration: 28500 Loss: 3.184361457824707 Perplexity: 24.151861498825053\n"
     ]
    }
   ],
   "source": [
    "for e in range(50):\n",
    "    batches = get_batches(in_text, out_text, flags.batch_size, flags.seq_size)\n",
    "    state_h, state_c = model.zero_state(flags.batch_size)\n",
    "\n",
    "    # Transfer data to GPU\n",
    "    state_h = state_h.to(device)\n",
    "    state_c = state_c.to(device)\n",
    "    for x, y in batches:\n",
    "        iteration += 1\n",
    "\n",
    "        # Tell it we are in training mode\n",
    "        model.train()\n",
    "\n",
    "        # Reset all gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Transfer data to GPU\n",
    "        x = torch.tensor(x).long().to(device)\n",
    "        y = torch.tensor(y).long().to(device)\n",
    "\n",
    "        logits, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "        loss = criterion(logits.transpose(1, 2), y)\n",
    "\n",
    "        state_h = state_h.detach()\n",
    "        state_c = state_c.detach()\n",
    "\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        # Perform back-propagation\n",
    "        loss.backward()\n",
    "        \n",
    "        _ = torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(), flags.gradients_norm)\n",
    "        \n",
    "        # Update the network's parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        if iteration % 100 == 0:\n",
    "            print('Epoch: {}/{}'.format(e, 200),\n",
    "                  'Iteration: {}'.format(iteration),\n",
    "                  'Loss: {}'.format(loss_value),\n",
    "                  'Perplexity: {}'.format(np.exp(loss_value)))\n",
    "\n",
    "        if iteration % 1000 == 0:\n",
    "            print(predict(device, model, flags.initial_words, n_vocab, vocab_to_int, int_to_vocab, top_k=5))\n",
    "            torch.save(model.state_dict(),\n",
    "                       'checkpoint_pt/model-{}.pth'.format(iteration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
